# 模型合并

“模型合并”通常指的是将两个或多个训练好的模型整合为一个单一模型的过程。这个过程可以基于不同的目的，比如为了简化部署流程、结合不同模型的优点或者是迁移学习等。不像集成学习方法需要运行多个模型，模型合并保留了单个模型相同的推理代价同时提供更好的性能。模型合并通常是直接操作模型参数，不需要运行模型。

## [mergekit](https://github.com/arcee-ai/mergekit)

**MergeKit的模型合并工具基于参数空间插值与模块化架构设计，通过整合多个预训练模型的参数或子模块，生成具备综合能力的复合模型**。其核心原理可拆解为以下层面：

### 一、参数空间插值：数学基础与算法实现

MergeKit的核心技术之一是参数空间插值，即在多个模型的参数向量之间进行加权组合。这一过程通过数学方法实现：

1. **向量归一化**：将输入模型的参数向量转换为单位长度，消除量纲差异对插值结果的影响。
2. **角度计算**：利用点积运算确定向量间的夹角，量化模型参数的几何关系。
3. **加权求和**：根据插值因子（如线性权重或球面插值参数）对原始向量进行加权，生成融合后的参数向量。

**典型算法**：

- **球面线性插值（SLERP）**：在保持向量方向性的同时进行平滑过渡，适用于合并两个相似模型。其优势在于保留高维空间中每个父模型的独特特征，避免线性插值可能导致的权重比例失衡。
- **TIES算法**：通过修剪冗余参数、解决符号冲突（当不同模型对同一参数提出相反调整时，选择主导变化方向）和选择性合并，实现多任务模型的整合。该方法可一次性合并多个模型，适用于需要多任务处理能力的场景。
- **DARE算法**：结合TIES思想，通过随机裁剪微调权重（将部分参数重置为原始值）和重新缩放权重（保持模型输出期望值不变），在减少参数冗余的同时维持模型性能。

### 二、模块化架构：灵活整合不同模型组件

MergeKit采用模块化设计，支持对模型的不同组件（如注意力层、前馈网络层）进行独立合并或替换。这种设计使得用户能够：

1. **混合专家（MoE）架构**：将基础模型的自注意力层和层归一化参数与多个专家模型的前馈网络层参数整合，构建稀疏激活的混合专家模型。例如，通过MergeKit的`mixtral`分支版本，可将Mistral-7B等模型转换为包含稀疏MoE层和路由器的架构。
2. **子模块替换**：针对特定任务需求，仅替换模型中的部分组件。例如，在对话模型中引入代码生成模型的注意力机制，以增强代码理解能力。

### 三、配置驱动：用户自定义合并策略

MergeKit通过YAML配置文件定义合并细节，用户可指定：

1. **模型来源**：参与合并的模型路径或名称。
2. **层范围**：需合并的模型层索引（如`[0, 32]`表示合并前32层）。
3. **合并方法**：选择SLERP、TIES或DARE等算法。
4. **参数设置**：插值因子、修剪比例等超参数。

**配置示例**：
```yaml
slices:
  - sources:
      - model: OpenPipe/mistral-ft-optimized-1218
        layer_range: [0, 32]
      - model: mlabonne/NeuralHermes-2.5-Mistral-7B
        layer_range: [0, 32]
    merge_method: slerp
    base_model: OpenPipe/mistral-ft-optimized-1218
    parameters:
      t:
        - filter: self_attn
          value: [0, 0.5, 0.3, 0.7, 1]
        - filter: mlp
          value: [1, 0.5, 0.7, 0.3, 0]
        - value: 0.5
```
此配置将两个Mistral-7B变体模型的前32层通过SLERP算法合并，其中自注意力层和MLP层的插值因子分别设置梯度值，其他层采用50%权重混合。

### 四、应用场景与优势

MergeKit的模型合并技术广泛应用于：

1. **多任务模型构建**：通过TIES算法合并多个特定任务模型（如对话、代码生成、数学推理），生成具备跨领域能力的复合模型。
2. **模型性能提升**：利用SLERP或DARE算法整合优势模型参数，增强模型在特定任务上的表现。例如，合并微调优化后的模型与基础模型，可提升幻觉检测性能。
3. **资源受限环境下的模型优化**：通过懒加载张量技术减少内存占用，支持在8GB显存的GPU上合并大型模型，降低硬件门槛。